<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>iFrames vs Web Components â€” Performance Dashboard</title>
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <header>
    <h1>iFrames vs Web Components â€” Performance Dashboard</h1>
    <div class="controls">
      <button id="refreshBtn">Refresh</button>
      <label>
        Filter implementation:
        <select id="implFilter">
          <option value="all">All</option>
          <option value="iframe">iframe</option>
          <option value="web-component">web-component</option>
        </select>
      </label>
      <label>
        Instances:
        <select id="countFilter">
          <option value="all">All</option>
          <option value="1">1</option>
          <option value="5">5</option>
          <option value="10">10</option>
        </select>
      </label>
    </div>
  </header>

  <main>
    <section class="intro">
      <h2>About this dashboard</h2>
      <p>
        This dashboard aggregates performance runs comparing two implementations of the same sports scoreboard widget:
        an iframe-based version and a Web Component. Test pages load <strong>1</strong>, <strong>5</strong>, or <strong>10</strong>
        instances to observe scalability. The automated tests (Playwright) open these pages, collect metrics, and save results here.
      </p>
      <div class="link-grid">
        <div class="link-card">
          <div class="link-title">iframe test pages</div>
          <ul>
            <li><a href="/iframe/tests/single.html" target="_blank" rel="noopener noreferrer">1 instance (single.html)</a></li>
            <li><a href="/iframe/tests/five.html" target="_blank" rel="noopener noreferrer">5 instances (five.html)</a></li>
            <li><a href="/iframe/tests/ten.html" target="_blank" rel="noopener noreferrer">10 instances (ten.html)</a></li>
          </ul>
        </div>
        <div class="link-card">
          <div class="link-title">web-component test pages</div>
          <ul>
            <li><a href="/web-component/tests/single.html" target="_blank" rel="noopener noreferrer">1 instance (single.html)</a></li>
            <li><a href="/web-component/tests/five.html" target="_blank" rel="noopener noreferrer">5 instances (five.html)</a></li>
            <li><a href="/web-component/tests/ten.html" target="_blank" rel="noopener noreferrer">10 instances (ten.html)</a></li>
          </ul>
        </div>
      </div>
    </section>

    <section>
      <h2>Comparison â€” median by implementation</h2>
      <div id="compareTable" class="compare"></div>
    </section>

    <section>
      <h2>Summary Statistics</h2>
      <div class="stats-guide">
        <p><strong>How to read these metrics:</strong></p>
        <ul>
          <li><strong>p50 (median):</strong> The typical performance â€” 50% of runs were faster, 50% slower. Use this to understand normal user experience.</li>
          <li><strong>p95:</strong> The "worst-case" performance â€” only 5% of runs were slower. Critical for understanding outliers and setting SLAs.</li>
          <li><strong>Ïƒ (standard deviation):</strong> Measures consistency. Lower values = more predictable performance; higher values = more variability.</li>
        </ul>
        <p><em>ðŸ’¡ Lower values are better for all metrics (time, memory). Higher FPS is better.</em></p>
      </div>
      <div id="summary" class="summary-grid">
        <div class="metric">
          <div class="label">Total runs</div>
          <div class="value" id="totalRuns">â€“</div>
        </div>
        <div class="metric stat-detail">
          <div class="label">Load Time (ms)</div>
          <div class="stat-row"><span class="stat-label">p50:</span><span class="value" id="loadP50">â€“</span></div>
          <div class="stat-row"><span class="stat-label">p95:</span><span class="value" id="loadP95">â€“</span></div>
          <div class="stat-row"><span class="stat-label">Ïƒ:</span><span class="value" id="loadStd">â€“</span></div>
        </div>
        <div class="metric stat-detail">
          <div class="label">FCP (ms)</div>
          <div class="stat-row"><span class="stat-label">p50:</span><span class="value" id="fcpP50">â€“</span></div>
          <div class="stat-row"><span class="stat-label">p95:</span><span class="value" id="fcpP95">â€“</span></div>
          <div class="stat-row"><span class="stat-label">Ïƒ:</span><span class="value" id="fcpStd">â€“</span></div>
        </div>
        <div class="metric stat-detail">
          <div class="label">Memory (MB)</div>
          <div class="stat-row"><span class="stat-label">p50:</span><span class="value" id="memP50">â€“</span></div>
          <div class="stat-row"><span class="stat-label">p95:</span><span class="value" id="memP95">â€“</span></div>
          <div class="stat-row"><span class="stat-label">Ïƒ:</span><span class="value" id="memStd">â€“</span></div>
        </div>
        <div class="metric stat-detail">
          <div class="label">FPS</div>
          <div class="stat-row"><span class="stat-label">p50:</span><span class="value" id="fpsP50">â€“</span></div>
          <div class="stat-row"><span class="stat-label">p95:</span><span class="value" id="fpsP95">â€“</span></div>
          <div class="stat-row"><span class="stat-label">Ïƒ:</span><span class="value" id="fpsStd">â€“</span></div>
        </div>
      </div>
    </section>

    <section>
      <h2>Runs</h2>
      <div id="emptyState" class="empty" hidden>No data files found yet. Run the tests to generate results.</div>
      <table id="resultsTable" class="results">
        <thead>
          <tr>
            <th>Time</th>
            <th>Implementation</th>
            <th>Instances</th>
            <th>Test Page</th>
            <th>Load (ms)</th>
            <th>FP (ms)</th>
            <th>FCP (ms)</th>
            <th>DOM Interactive (ms)</th>
            <th>DOM Complete (ms)</th>
            <th>Avg Memory (MB)</th>
            <th>Avg FPS</th>
          </tr>
        </thead>
        <tbody></tbody>
      </table>
    </section>

    <section class="metrics-reference">
      <h2>Metrics Reference</h2>
      <p class="reference-intro">Understanding what each performance metric measures and why it matters for user experience and application performance.</p>
      
      <div class="metric-cards">
        <div class="metric-card">
          <h3>Load Time</h3>
          <div class="metric-description">
            <p><strong>What it measures:</strong> Time from navigation start until the page <code>load</code> event fires (all resources loaded).</p>
            <p><strong>Why it matters:</strong> Indicates when the page is fully interactive and all scripts, styles, and images have finished loading. Critical for perceived performance and user engagement.</p>
          </div>
        </div>

        <div class="metric-card">
          <h3>First Paint (FP)</h3>
          <div class="metric-description">
            <p><strong>What it measures:</strong> Time until the browser renders the first pixel on screen - any visual change from blank.</p>
            <p><strong>Why it matters:</strong> First signal to the user that the page is loading. Reduces perceived latency and shows progress.</p>
          </div>
        </div>

        <div class="metric-card">
          <h3>First Contentful Paint (FCP)</h3>
          <div class="metric-description">
            <p><strong>What it measures:</strong> Time until the browser renders the first piece of DOM content (text, image, SVG, or non-white canvas).</p>
            <p><strong>Why it matters:</strong> Part of Google's Core Web Vitals. Shows when users can first see meaningful content. Directly impacts user perception of speed.</p>
          </div>
        </div>

        <div class="metric-card">
          <h3>DOM Interactive</h3>
          <div class="metric-description">
            <p><strong>What it measures:</strong> Time when the HTML document has been completely parsed and the DOM tree is built (but resources may still be loading).</p>
            <p><strong>Why it matters:</strong> Indicates when JavaScript can start manipulating the DOM. Shows parser performance and blocking script impact.</p>
          </div>
        </div>

        <div class="metric-card">
          <h3>DOM Complete</h3>
          <div class="metric-description">
            <p><strong>What it measures:</strong> Time when the page and all sub-resources (images, scripts, styles) have finished loading.</p>
            <p><strong>Why it matters:</strong> Marks the end of page loading. All synchronous resources are available. Fires just before the <code>load</code> event.</p>
          </div>
        </div>

        <div class="metric-card">
          <h3>Memory Usage</h3>
          <div class="metric-description">
            <p><strong>What it measures:</strong> Average JavaScript heap size (in MB) during page operation, sampled over time.</p>
            <p><strong>Why it matters:</strong> High memory usage can cause performance degradation, crashes on memory-constrained devices, and poor battery life. Critical for mobile users.</p>
          </div>
        </div>

        <div class="metric-card">
          <h3>Frame Rate (FPS)</h3>
          <div class="metric-description">
            <p><strong>What it measures:</strong> Average frames per second during page updates and animations, measured via <code>requestAnimationFrame</code>.</p>
            <p><strong>Why it matters:</strong> Smooth animations require 60 FPS. Lower frame rates appear janky and hurt user experience. Indicates rendering performance and main thread blocking.</p>
          </div>
        </div>

        <div class="metric-card">
          <h3>Statistical Metrics</h3>
          <div class="metric-description">
            <p><strong>p50 (median):</strong> The middle value â€” half of runs performed better, half worse. Best indicator of typical user experience.</p>
            <p><strong>p95 (95th percentile):</strong> Only 5% of runs were slower. Critical for SLAs and understanding worst-case scenarios that affect real users.</p>
            <p><strong>Ïƒ (standard deviation):</strong> Measures variability. Low Ïƒ = consistent, predictable performance. High Ïƒ = unpredictable, potentially problematic.</p>
            <p><strong>Why percentiles matter:</strong> Averages hide outliers. A page that loads in 100ms 99 times and 10,000ms once has an average of 199ms, but most users saw 100ms. The p95 would be 10,000ms, revealing the true problem.</p>
          </div>
        </div>
      </div>

      <div class="methodology-note">
        <h3>Testing Methodology</h3>
        <p>All tests run via Playwright in Chromium using the Performance API. Multiple runs (17 per scenario) capture variance. Cache-busting ensures fresh loads. Metrics collected during actual user interactions, not synthetic benchmarks.</p>
        <p><strong>Test scenarios:</strong> 1, 5, and 10 concurrent widget instances to measure scalability and multi-instance overhead.</p>
      </div>
    </section>
  </main>

  <script src="app.js"></script>
</body>
</html>
